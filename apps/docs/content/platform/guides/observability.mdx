---
title: Observability Quick Start Guide
description: Guia prÃ¡tico para comeÃ§ar a usar o sistema de observabilidade do Kaven - setup rÃ¡pido, interpretaÃ§Ã£o de mÃ©tricas e troubleshooting
date: 2026-01-06
author: Chris + Claude Sonnet 4.5
version: 3.0.0
tags:
  [guide, observability, monitoring, quick-start, dashboard, troubleshooting]
---

# Observability Quick Start Guide

> Guia prÃ¡tico para configurar e usar o sistema de observabilidade do Kaven em 10 minutos

## ğŸš€ Setup RÃ¡pido (5 minutos)

### 1. Iniciar Stack de Monitoramento

```bash
# Na raiz do projeto
docker-compose -f docker-compose.monitoring.yml up -d

# Verificar se todos os containers estÃ£o rodando
docker-compose -f docker-compose.monitoring.yml ps
```

**Containers iniciados:**

- âœ… Prometheus (porta 9090)
- âœ… Grafana (porta 3001)
- âœ… Alertmanager (porta 9093)
- âœ… PostgreSQL Exporter (porta 9187)
- âœ… Redis Exporter (porta 9121)

### 2. Configurar Sentry (Opcional)

```bash
# Adicionar ao .env
SENTRY_DSN=https://your-key@sentry.io/project-id

# Reiniciar API
cd apps/api
pnpm dev
```

### 3. Acessar Dashboards

| Interface           | URL                                 | Credenciais      |
| ------------------- | ----------------------------------- | ---------------- |
| **Dashboard Kaven** | http://localhost:3000/observability | Login normal     |
| **Prometheus**      | http://localhost:9090               | Sem autenticaÃ§Ã£o |
| **Grafana**         | http://localhost:3001               | admin/admin      |
| **Alertmanager**    | http://localhost:9093               | Sem autenticaÃ§Ã£o |

---

## ğŸ“Š Usando o Dashboard

### VisÃ£o Geral

O dashboard de observabilidade do Kaven mostra **8 cards principais** organizados em 2 seÃ§Ãµes:

**Golden Signals (4 cards):**

1. ğŸ• **Latency** - Tempo de resposta (p50, p95, p99)
2. ğŸ“ˆ **Traffic** - Volume de requisiÃ§Ãµes
3. âŒ **Errors** - Taxa de erros
4. ğŸ’» **Saturation** - Uso de CPU e memÃ³ria

**Node.js Metrics (4 cards):** 5. âš¡ **Event Loop Lag** - SaÃºde do event loop 6. ğŸ§  **Memory Heap** - Uso de memÃ³ria heap 7. ğŸ“Œ **Active Handles** - Recursos ativos 8. ğŸ”„ **Active Requests** - RequisiÃ§Ãµes em andamento

### Auto-Refresh

O dashboard atualiza automaticamente a cada **5 segundos**. VocÃª verÃ¡:

- âœ… Skeleton loaders durante carregamento
- âœ… Cores semÃ¢nticas indicando status
- âœ… Valores em tempo real

---

## ğŸ¨ Interpretando as Cores

### Status Visual

| Cor                      | Significado | AÃ§Ã£o NecessÃ¡ria          |
| ------------------------ | ----------- | ------------------------ |
| ğŸŸ¢ **Verde** (Success)   | SaudÃ¡vel    | Nenhuma                  |
| ğŸŸ¡ **Amarelo** (Warning) | AtenÃ§Ã£o     | Monitorar                |
| ğŸ”´ **Vermelho** (Error)  | CrÃ­tico     | Investigar imediatamente |
| ğŸ”µ **Azul** (Info)       | Informativo | Nenhuma                  |
| ğŸŸ£ **Roxo** (Secondary)  | Neutro      | Nenhuma                  |

### Thresholds por MÃ©trica

#### Event Loop Lag

- ğŸŸ¢ **< 10ms** - SaudÃ¡vel
- ğŸŸ¡ **10-50ms** - AtenÃ§Ã£o (possÃ­vel cÃ³digo bloqueante)
- ğŸ”´ **> 50ms** - CrÃ­tico (event loop bloqueado)

#### Memory Heap

- ğŸŸ¢ **< 70%** - SaudÃ¡vel
- ğŸŸ¡ **70-85%** - AtenÃ§Ã£o (monitorar crescimento)
- ğŸ”´ **> 85%** - CrÃ­tico (risco de memory leak)

#### Active Handles

- ğŸŸ¢ **< 100** - SaudÃ¡vel
- ğŸŸ¡ **100-200** - AtenÃ§Ã£o
- ğŸ”´ **> 200** - CrÃ­tico (possÃ­vel leak de recursos)

#### Active Requests

- ğŸŸ¢ **< 10** - SaudÃ¡vel
- ğŸŸ¡ **10-50** - AtenÃ§Ã£o (carga moderada)
- ğŸ”´ **> 50** - CrÃ­tico (sistema sobrecarregado)

#### Error Rate

- ğŸŸ¢ **< 1%** - SaudÃ¡vel
- ğŸŸ¡ **1-5%** - AtenÃ§Ã£o
- ğŸ”´ **> 5%** - CrÃ­tico (problema sistÃªmico)

---

## ğŸ” CenÃ¡rios Comuns

### CenÃ¡rio 1: AplicaÃ§Ã£o Lenta

**Sintomas no Dashboard:**

- ğŸ”´ Latency p95 > 2000ms
- ğŸ”´ Event Loop Lag > 50ms
- ğŸŸ¡ Active Requests > 20

**DiagnÃ³stico:**

```bash
# Ver performance profiling
curl http://localhost:8000/api/diagnostics/performance

# Verificar event loop lag
curl http://localhost:8000/api/observability/advanced | jq '.nodejs.eventLoopLag'
```

**Causas Comuns:**

1. CÃ³digo bloqueante (operaÃ§Ãµes sÃ­ncronas)
2. Queries lentas no banco
3. APIs externas lentas
4. Memory leak causando GC frequente

**SoluÃ§Ã£o:**

```typescript
// âŒ ERRADO - Bloqueante
const data = fs.readFileSync('large-file.json');
const result = heavyComputation(data);

// âœ… CORRETO - AssÃ­ncrono
const data = await fs.promises.readFile('large-file.json');
const result = await heavyComputationAsync(data);
```

---

### CenÃ¡rio 2: Memory Leak

**Sintomas no Dashboard:**

- ğŸ”´ Memory Heap > 85% e crescendo
- ğŸŸ¡ Active Handles crescendo constantemente
- ğŸŸ¢ Event Loop Lag normal

**DiagnÃ³stico:**

```bash
# Ver memory profiling
curl http://localhost:8000/api/diagnostics/memory

# Monitorar crescimento
watch -n 5 'curl -s http://localhost:8000/api/diagnostics/memory | jq ".heapUsagePercent"'
```

**Causas Comuns:**

1. Event listeners nÃ£o removidos
2. Timers/intervals nÃ£o limpos
3. Cache sem limite de tamanho
4. Closures retendo referÃªncias

**SoluÃ§Ã£o:**

```typescript
// âœ… Sempre limpar event listeners
useEffect(() => {
  const handler = () => {
    /* ... */
  };
  window.addEventListener('resize', handler);

  return () => window.removeEventListener('resize', handler);
}, []);

// âœ… Limpar timers
useEffect(() => {
  const interval = setInterval(() => {
    /* ... */
  }, 1000);

  return () => clearInterval(interval);
}, []);
```

---

### CenÃ¡rio 3: Alta Taxa de Erros

**Sintomas no Dashboard:**

- ğŸ”´ Error Rate > 5%
- ğŸ”´ Error Requests crescendo
- ğŸŸ¢ Latency normal

**DiagnÃ³stico:**

```bash
# Ver erros no Sentry (se configurado)
# Acessar: https://sentry.io/your-project

# Ver logs de erro
docker-compose logs api | grep ERROR

# Ver distribuiÃ§Ã£o de status HTTP
curl http://localhost:8000/api/observability/advanced | jq '.httpDetails.statusDistribution'
```

**Causas Comuns:**

1. API externa fora do ar
2. ValidaÃ§Ã£o de dados falhando
3. Erro de configuraÃ§Ã£o
4. Database connection pool esgotado

**SoluÃ§Ã£o:**

```typescript
// âœ… Usar Circuit Breaker para APIs externas
import { stripeCircuitBreaker } from '@/lib/circuit-breaker';

const payment = await stripeCircuitBreaker.execute(async () => {
  return await stripe.paymentIntents.create(data);
});
```

---

### CenÃ¡rio 4: Sistema Sobrecarregado

**Sintomas no Dashboard:**

- ğŸ”´ CPU > 90%
- ğŸ”´ Active Requests > 50
- ğŸ”´ Latency p99 > 5000ms
- ğŸŸ¡ Memory crescendo

**DiagnÃ³stico:**

```bash
# Ver health check detalhado
curl http://localhost:8000/api/diagnostics/health

# Ver mÃ©tricas de hardware
curl http://localhost:8000/api/observability/metrics | grep cpu_usage
```

**AÃ§Ãµes Imediatas:**

1. Escalar horizontalmente (mais instÃ¢ncias)
2. Ativar rate limiting
3. Habilitar cache
4. Investigar queries N+1

---

## ğŸ› ï¸ Troubleshooting AvanÃ§ado

### Prometheus NÃ£o EstÃ¡ Coletando MÃ©tricas

```bash
# 1. Verificar targets
curl http://localhost:9090/api/v1/targets | jq

# 2. Ver logs do Prometheus
docker-compose -f docker-compose.monitoring.yml logs prometheus

# 3. Testar endpoint de mÃ©tricas
curl http://localhost:8000/api/observability/metrics

# 4. Verificar configuraÃ§Ã£o
docker exec kaven-prometheus cat /etc/prometheus/prometheus.yml
```

**SoluÃ§Ã£o:** Verificar se API estÃ¡ acessÃ­vel de dentro do container Docker.

---

### Alertmanager NÃ£o Envia NotificaÃ§Ãµes

```bash
# 1. Ver alertas ativos
curl http://localhost:9093/api/v2/alerts

# 2. Ver configuraÃ§Ã£o
curl http://localhost:9093/api/v1/status

# 3. Testar webhook manualmente
curl -X POST http://localhost:8000/api/webhooks/alerts \
  -H "Content-Type: application/json" \
  -d '{"alerts": [{"status": "firing", "labels": {"alertname": "Test"}}]}'

# 4. Ver logs
docker-compose -f docker-compose.monitoring.yml logs alertmanager
```

**SoluÃ§Ã£o:** Verificar se webhook URL estÃ¡ correta e acessÃ­vel.

---

### Grafana NÃ£o Conecta ao Prometheus

```bash
# 1. Verificar datasource
docker exec kaven-grafana cat /etc/grafana/provisioning/datasources/prometheus.yml

# 2. Testar conectividade
docker exec kaven-grafana ping prometheus

# 3. Ver logs
docker-compose -f docker-compose.monitoring.yml logs grafana
```

**SoluÃ§Ã£o:** Verificar se nome do serviÃ§o estÃ¡ correto no datasource (deve ser `prometheus`, nÃ£o `localhost`).

---

## ğŸ“ˆ Queries Prometheus Ãšteis

### Performance

```promql
# Request rate (Ãºltimos 5 minutos)
rate(kaven_http_requests_total[5m])

# P95 latency
histogram_quantile(0.95, rate(kaven_http_request_duration_seconds_bucket[5m]))

# Error rate
rate(kaven_http_requests_total{status=~"5.."}[5m]) / rate(kaven_http_requests_total[5m])
```

### Recursos

```promql
# CPU usage
kaven_hardware_cpu_usage_percent

# Memory usage
kaven_hardware_memory_usage_percent

# Disk usage
kaven_hardware_disk_usage_percent
```

### Business Metrics

```promql
# User registrations per hour
rate(kaven_user_registrations_total[1h])

# Login success rate
rate(kaven_login_attempts_total{status="success"}[5m]) / rate(kaven_login_attempts_total[5m])

# Payment volume
rate(kaven_payments_total[1h])
```

### Circuit Breaker

```promql
# Circuit breaker state (0=closed, 1=open, 2=half-open)
kaven_circuit_breaker_state

# Circuit breaker failures
rate(kaven_circuit_breaker_failures_total[5m])
```

---

## ğŸ¯ Best Practices

### 1. Monitoramento Proativo

âœ… **FaÃ§a:**

- Configure alertas para mÃ©tricas crÃ­ticas
- Monitore tendÃªncias, nÃ£o apenas valores absolutos
- Revise dashboards semanalmente
- Documente incidentes e resoluÃ§Ãµes

âŒ **Evite:**

- Esperar usuÃ¡rios reportarem problemas
- Ignorar alertas de warning
- Desabilitar alertas "chatos"

### 2. Performance

âœ… **FaÃ§a:**

- Mantenha Event Loop Lag < 10ms
- Use async/await para I/O
- Implemente cache para dados frequentes
- Use Circuit Breaker para APIs externas

âŒ **Evite:**

- OperaÃ§Ãµes sÃ­ncronas pesadas
- Queries N+1
- Loops bloqueantes
- Timers/intervals sem cleanup

### 3. MemÃ³ria

âœ… **FaÃ§a:**

- Monitore heap usage regularmente
- Implemente limites em caches
- Limpe event listeners
- Use WeakMap/WeakSet quando apropriado

âŒ **Evite:**

- Caches sem limite
- Closures desnecessÃ¡rias
- Listeners globais sem cleanup
- Objetos grandes em memÃ³ria

### 4. Alertas

âœ… **FaÃ§a:**

- Configure mÃºltiplos nÃ­veis de severidade
- Use routing inteligente (Slack para critical, email para warning)
- Documente runbooks para cada alerta
- Teste alertas regularmente

âŒ **Evite:**

- Alertas muito sensÃ­veis (muito ruÃ­do)
- Alertas muito lentos (problema jÃ¡ crÃ­tico)
- NotificaÃ§Ãµes sem contexto
- Alertas sem aÃ§Ã£o clara

---

## ğŸ“š PrÃ³ximos Passos

### Aprofundar Conhecimento

1. **ReferÃªncia TÃ©cnica Completa**
   - Leia [Observability & Monitoring](/platform/observability)
   - Entenda todas as 30+ mÃ©tricas disponÃ­veis
   - Configure dashboards Grafana customizados

2. **ConfiguraÃ§Ã£o AvanÃ§ada**
   - Configure Slack/Email notifications
   - Crie alert rules customizadas
   - Implemente SLOs (Service Level Objectives)

3. **IntegraÃ§Ã£o com CI/CD**
   - Monitore deploys
   - Track error rate pÃ³s-deploy
   - Rollback automÃ¡tico em caso de problemas

### Recursos Adicionais

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Dashboards](https://grafana.com/grafana/dashboards/)
- [Sentry Best Practices](https://docs.sentry.io/platforms/javascript/guides/node/)
- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)

---

## ğŸ†˜ Precisa de Ajuda?

### Checklist de Troubleshooting

- [ ] Todos os containers Docker estÃ£o rodando?
- [ ] API estÃ¡ respondendo em http://localhost:8000?
- [ ] Prometheus consegue acessar `/api/observability/metrics`?
- [ ] Grafana estÃ¡ conectado ao Prometheus?
- [ ] Alertmanager estÃ¡ recebendo alertas?
- [ ] Webhooks estÃ£o configurados corretamente?
- [ ] Sentry DSN estÃ¡ configurado (se usando)?

### Comandos de DiagnÃ³stico RÃ¡pido

```bash
# Status geral
docker-compose -f docker-compose.monitoring.yml ps

# Logs de todos os serviÃ§os
docker-compose -f docker-compose.monitoring.yml logs -f

# Reiniciar stack
docker-compose -f docker-compose.monitoring.yml restart

# Parar e remover tudo
docker-compose -f docker-compose.monitoring.yml down -v
```

---

## ğŸ“ Changelog

### v3.0.0 (2026-01-06)

- âœ… Atualizado para refletir stack completo implementado
- âœ… Adicionado guia de Circuit Breaker
- âœ… Adicionado troubleshooting de Sentry
- âœ… Adicionado queries Prometheus Ãºteis
- âœ… Reorganizado como guia prÃ¡tico (nÃ£o referÃªncia tÃ©cnica)

### v2.0.0 (anterior)

- Golden Signals e Node.js metrics
- Dashboard frontend

---

**Tipo:** Guia PrÃ¡tico  
**Complementa:** [Observability & Monitoring](/platform/observability) (ReferÃªncia TÃ©cnica)  
**Tempo de Leitura:** ~15 minutos
